{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Emzkebg69p4G"
   },
   "source": [
    "# Applying Naive Bayes to N-Grams for Markov Chain Generation / Text Prediction\n",
    "\n",
    "*Disclaimer: the code in this notebook is excerpted from the source code and is functional in and of itself; access the source code [here](https://github.com/duncanmazza/ml_ngrams).*\n",
    "\n",
    "## Introduction and Mathematical Definitions\n",
    "Many text prediction models utilize some implementation of a n-gram model where consecutive sequences of words of a specified length n in a text can used to predict the next word in a sequence. In this project, I sought to implement a n-gram-like model that utilizes a continuously generated rolling sequence of words and a graph of the directional distances between words to encode n-grams rather than explicitly use n-grams themselves. My implementation was heavily based on the work of Henrique X. Goulart, Mauro D. L. Tosi, Daniel Soares-Gonc Ì§ alves, and Rodrigo F. Maiaand Guilherme Wachs-Lopes in their paper \"Hybrid Model For Word Prediction Using NaiveBayes and Latent Information\"[1], although significant modifications were made to appropriately scope my implementation for the weeklong project.\n",
    "\n",
    "In the paper, the authors present a method of text prediction that combines  latent semantic analysis (LSA) and naive Bayes along with an algorithm to optimze model hyperparameters to achieve an error rate far lower than LSA or naive Bayes alone. I chose to implement my own variation of just the naive Bayes portion of their model.\n",
    "\n",
    "### Naive Bayes Graph\n",
    "Following the definition in [1], there exists a set of directional graphs $\\{ G_0, G_1, ..., G_{d-1} \\}$ for a vector of words $\\mathbf{V}=[w_1, w_2, ..., w_d]^T$ where the vocabulary $\\mathbf{V}_v$ is given by $set(\\mathbf{V})$ and the nodes of graph are comprised by the vocabulary. The directional edges in graph $G_i$ between the nodes are positive integer values that represent the number of occurances of the parent node at distance $i$ from the child node in the text.\n",
    "\n",
    "For example, in the text:\n",
    "``` \n",
    "the quick brown fox jumped over the lazy dog the quick cat did not jump over the lazy dog\n",
    "```\n",
    "there is a distance 0 from parent node `the` to child node `quick`, and this occurs twice in the text; therefore, in graph $G_0$, the edge from `the` to `quick` has a value of 2. For another example, there is a distance 1 between parent node `jump` and child node `the` that occurs twice in the text; therefore, the edge from `jump` to `the` in graph $G_1$ has a value of 2. The utilization of this graph is discussed in the next section; following is the description of how I created it.\n",
    "\n",
    "As a step of pre-processing for the text passed into my model, I generated and cached (in RAM) a list of graphs $[G_0, G_1, ..., G_{max\\_chain}]$ where `max_chain` is a model parameter (discussed later). These graphs are stored as 2-dimensional square sparse matrices $\\mathbf{M}$ of type `scscipy.sparse.dok_matrix` (dictionary of keys based sparse matrix), where indices $(i, j)$ correspond to a pair of vocabulary words $(V_{v_i}, V_{v_j})$; a dictionary called `vocab_to_matrix` stores the vocabulary words' respective index values. The matching of a vocabulary word to an index value is arbitrary as long as it is consistent across both axes and all graphs. Two nested for-loops iterate over a list of all words in $\\mathbf{V}$ and over windows of size `max_chain`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyepgP88Nde7"
   },
   "outputs": [],
   "source": [
    "# api/book.py\n",
    "from scipy.sparse import dok_matrix\n",
    "\"\"\"\n",
    "Follows is an excerpted method of the Book class that creaates the aforementioned Bayesian graph\n",
    "\"\"\"\n",
    "def _make_bayesian_graphs(self):\n",
    "    \"\"\"\n",
    "    Build the list of bayesian graphs, where the graph at index i represents the directional graph between words in\n",
    "    the text vector (self.tokens), where the nodes are vocabulary words and the directional edges are the number of\n",
    "    occurrences of the parent node separated by the child node by distance i in the text.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    self.graphs = []\n",
    "    for i in range(self.max_chain):  # each item in self.graphs is a sparse matrix (M) where M[i, j] represents the\n",
    "        # value of the directional edge from vocab_to_matrix[i] to vocab_to_matrix[j]; i and j are integers that\n",
    "        # correspond to words per the dictionary vocab_to_matrix (populated below) with keys of words and values of\n",
    "        # the words' corresponding indices in the matrix\n",
    "        self.graphs.append(dok_matrix((self.vocabulary_size, self.vocabulary_size), dtype=int))\n",
    "    self.vocab_to_matrix = {}\n",
    "    for i, vocab in enumerate(self.vocabulary.keys()):\n",
    "        self.vocab_to_matrix[vocab] = i  # record what matrix index corresponds to which word; values are arbitrary\n",
    "        # as long as they remain unchanged and are unique to their respective key\n",
    "    for t, token in enumerate(self.tokens): # iterate the search window over every word in the text\n",
    "        # enable the last `self.max_chain` words in the text serve as the basis for the search window by shortening\n",
    "        # the search window so that it doesn't overextend the list of words\n",
    "        if self.max_chain + t >= self.num_words:\n",
    "            upper_limit = self.num_words - t\n",
    "        else:\n",
    "            upper_limit = self.max_chain + 1\n",
    "        for c in range(1, upper_limit):  # iterate over search window\n",
    "            self.graphs[c - 1][self.vocab_to_matrix[token], self.vocab_to_matrix[self.tokens[c + t]]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSVWJscMOpv5"
   },
   "source": [
    "To establish some notation for the values in this graph, let $w^d_{i, j}$ represent the occurances of word i separated from word j by distance d in the document. Note that distances are only calculated in a forward direction through the vector of word tokens.\n",
    "\n",
    "### Naive Bayes Algorithm\n",
    "\n",
    "The naive Bayes algorithm is generally stated as[1]:\n",
    "$$\n",
    "P(e|c_1, c_2,...,c_n) = \\frac{P(e) \\prod_{i=1}^{n}{P(c_i|e)}}{\\gamma}\n",
    "$$\n",
    "where $e$ represents an effect that is given by of a set of causes $\\{c_1, c_2, ..., c_n\\}$ that are independent when conditioned on $e$, and $\\gamma$ is a normalization factor.\n",
    "\n",
    "In the model, we will consider an implementation[1] of the naive Bayes algorithm that utilizes the frequency of distances between words to calculate the likelihood term, $P(c_i|e)$, of the Bayes algorithm:\n",
    "$$\n",
    "B_j = P(sugg_j|\\mathbf{prev}_{0:n-1}) = \\frac{P(sugg_j)\\prod_{i=0}^{n-1}{P^i(prev_j|sugg_i)}}{\\gamma}\n",
    "$$\n",
    "where $\\mathbf{prev}_{0:n-1}$ is the n previous words (in descending order) to the suggested word $sugg_j$, $P(sugg_j)$ is just the term frequency of $sugg_j$ in the text, and the likelihood is given by:\n",
    "$$\n",
    "P^i(prev_j|sugg_i) = \\frac{w^i_{prev_i, sugg_j}}{\\sum_{j}{w^i_{prev_i, sugg_j}}}\n",
    "$$\n",
    "This is interpreted as: the probability of the previous word $prev_i$ occuring at distance $i$ downstream from the suggested word $sugg_j$ is given by the number of occurances of that relationship divided by the sum of the number of occurances of $prev_j$ downstream from all suggested words. Therefore, $B_j$ is read as the probability of the suggested word given the n words preceding it, and it is calculated using the distances between the suggested and previous words in the text.\n",
    "\n",
    "The paper[1] then goes on to integrate parameters into this expression that are later optimized; this is where I deviate from the prior work.\n",
    "\n",
    "To account for the fact $P^i(prev_j|sugg_i)$ will often evaluate to $0$, I modified Bayes' algorithm by splitting up the product into a sum of terms using logarithms, and inserted an additive term ($\\alpha$) into the logarithmic arguments to achieve Laplace smoothing:\n",
    "\n",
    "$$\n",
    "\\log{B_j'} = \\log{(P(sugg_j) + \\alpha)} + \\sum_{i=0}^{n-1}{\\log{(P^i(prev_i|sugg_j) + \\alpha)}}\n",
    "\\\\\n",
    "\\therefore \\space \\space B_j' = e^{\\log{(P(sugg_j) + \\alpha)} + \\sum_{i=0}^{n-1}{\\log{(P^i(prev_i|sugg_j))}}}\n",
    "$$\n",
    "\n",
    "($B'$ is used because the probabilities are altered with the use of $\\alpha$). In the case that $P^i(prev_j|sugg_i) = \\infty$, I artificially set the value to 0. Note that $\\gamma$ has been dropped from this equation; this is because each $B_j$ is an element of a vector $\\mathbf{B'} = \\frac{[B_1',B_2',...,B_d']^T}{||[B_1',B_2',...,B_d']||_1}$ which is normalized using a L1 norm such that it satisfies the property of PMFs that the sum of the probabiliies is equal to one: $||\\mathbf{B'}||_1=1$. Therefore, $\\gamma$ is no longer needed.\n",
    "\n",
    "To implement this naive Bayes algorithm, I wrote several helper functions, a few of which are outlined below:\n",
    "- `_p_d_i_j` - returns $P^d(i|j)$ from arguments $d$, $i$, $j$, and $\\mathbf{sugg}$ (the set of suggested words in arbitrary order).\n",
    "- `_p_s` - returns $P(sugg_j)$.\n",
    "- `generate_cond_prob_arr` - returns $\\mathbf{B}'$ (the vector of conditional probabilities) given a vector of suggested words $\\mathbf{sugg}_j$ of length $k$ and a vector of previous words $\\mathbf{prev}_{0:n-1}$ where $\\mathbf{B}'$ is of length $k$ and $B_j'$ corresponds to the probability associated with $sugg_j$.\n",
    "\n",
    "The suggested word with the highest probability is chosen as the next work in the Markov chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cozxAkPpZMK9"
   },
   "outputs": [],
   "source": [
    "# api/book.py\n",
    "\"\"\"\n",
    "Follows are excerpted methods of the Book class that implement the aforementioned functionality\n",
    "\"\"\"\n",
    "def _p_s(self, _s):\n",
    "    \"\"\"\n",
    "    Returns P(s)\n",
    "    :param _s: word in book vocabulary\n",
    "    :return: p(s) where p(s) = (number of occurances of s) / (number of words in book)\n",
    "    \"\"\"\n",
    "    return self.vocabulary[_s] / self.num_words\n",
    "\n",
    "def _p_d_i_j(self, d, _p, _s, tuple_s):\n",
    "    \"\"\"\n",
    "    Returns P^d(i, j)\n",
    "    :param d: distance\n",
    "    :param _p: previous word\n",
    "    :param _j: suggested word\n",
    "    :param tuple_s: tuple of all possible suggested words\n",
    "    :return: (wight of graph d from _p to _s) / (sum of wights of graph d from _p to all list_s); if the denominator\n",
    "     is 0, then return 0 to avoid dividing by 0\n",
    "    \"\"\"\n",
    "    val_w_d_p_s = self.query_graph(d, _p, _s)\n",
    "    # first, check caches for whether these values have already been queried\n",
    "    val_sum_w_d_p_list_s = self.sum_w_d_p_list_s_cache.get((d, _p, tuple_s))\n",
    "    if val_sum_w_d_p_list_s is None:\n",
    "        val_sum_w_d_p_list_s = 0\n",
    "        for _s_ in tuple_s: val_sum_w_d_p_list_s += self.query_graph(d, _p, _s_)\n",
    "        self.sum_w_d_p_list_s_cache[\n",
    "            (d, _p, tuple_s)] = val_sum_w_d_p_list_s  # store so that this calculation isn't redone\n",
    "    if val_sum_w_d_p_list_s == 0:  # return 0 to avoid dividing by 0\n",
    "        return 0\n",
    "    else:\n",
    "        return val_w_d_p_s / val_sum_w_d_p_list_s\n",
    "\n",
    "def generate_cond_prob_arr(self, tuple_s, list_p_forward):\n",
    "    \"\"\"\n",
    "    :param tuple_s: list of unique suggested words (order is arbitrary but must be maintained so words can\n",
    "     correspond to values in cond_prob_arr)\n",
    "    :param list_p_rev: (ordered) list of previous words preceding suggested word\n",
    "    :return: numpy array of length len(list_s_set) that contains the conditional probabilities\n",
    "    \"\"\"\n",
    "    cond_prob_arr = np.zeros((len(tuple_s)))\n",
    "\n",
    "    # calculate create an array that has the previous words in reverse order\n",
    "    list_p_rev = [list_p_forward[p] for p in range(len(list_p_forward) - 1, -1, -1)]\n",
    "\n",
    "    # iterate over the suggested words\n",
    "    for j, _s in enumerate(tuple_s):\n",
    "        prior_val = self._p_s(_s)\n",
    "        likelihood_arr = np.ones((len(list_p_rev))) * self.alpha  # initialize with alpha value\n",
    "        for i in range(len(list_p_rev)):\n",
    "            likelihood_arr[i] += self._p_d_i_j(i, list_p_rev[i], _s, tuple_s)\n",
    "        log_sum_likelihood_val = np.sum(np.log(likelihood_arr))\n",
    "        cond_prob_arr[j] = np.exp(np.log(prior_val + self.alpha) + log_sum_likelihood_val)\n",
    "\n",
    "    return cond_prob_arr / np.sum(cond_prob_arr)  # normalize so values sum to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNf8SrzUaDW8"
   },
   "source": [
    "While the overhead to calculate the graph is non-trivial, it is a one time operation. Once that has been calculated, applying the naive Bayes algorithm in my implementation takes a trivial amount of time thanks to the fact that I cache the $\\sum_{j}{w^i_{prev_i, sugg_j}} $ term so that it is only calculated once for each loop that generates another word; additionally, the implementation only consider a simple random sample of 100 suggested words if the number of suggested words is >100. Further optimizations of this algorithm would include parallelization, as calculating each $P^i(prev_i|sugg_j)$ is an independent operation.\n",
    "\n",
    "An important consideration of this implementation is that the maximum number of previous words considered is given by a parameter `max_chains`, where the length of the previous words considered, `len(list_p)`, is equal to `max_chains`. This speeds up the execution of the algorithm, but limits the contextual inference of the algorithm to the previous `max_chain` words in a given Markov chain.\n",
    "\n",
    "## Data Wrangling\n",
    "The main routine of this code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97MbItlWeIh0"
   },
   "outputs": [],
   "source": [
    "# main.py\n",
    "from api.librarian import Librarian\n",
    "\n",
    "book_list = (\n",
    "    ('Frankenstein', 'Mary Wollstonecraft (Godwin) Shelley'),\n",
    "    # ('Watersprings', 'Arthur Christopher Benson')\n",
    "    # ... any other book in Project Gutenberg\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # acquire books from the book list\n",
    "    librarian = Librarian(book_list, global_truncate=0.4, global_alpha=1, global_max_chain=2)\n",
    "    while True:\n",
    "        for book_name in librarian.acquired_books:\n",
    "            book = librarian.acquired_books[book_name]\n",
    "            print(\"Appling naive bayes to: {}\".format(book_name))\n",
    "            book.apply_naive_bayes(extend_by=15)\n",
    "\n",
    "        print('Generate another sample? (y or Y) -> ')\n",
    "        while True:\n",
    "            i = input()\n",
    "            if i == \"y\" or \"Y\":\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4pP95XBeNeO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The book list is a tuple of 2-element tuples whose first element is a book name and whose second element is the author of the book; these books must be available in the Project Gutenberg index. Initializing a Librarian object with this list prompts the \"librarian\" to fetch, parse, and cache (to disk) the full texts of the specified books; it does this via `Book` objects:\n",
    "\n",
    "The `librarian` object has an attribute `acquired_books` which is a dictionary keyed by book names/authors and whose values are `Book` objects. A `Book` object upon initialization will perform a variety of parsing tasks that include tokenizing the text, storing a dictionary of vocabulary words and all possible corresponding words that occur after that word in the text, creating the aforementioned Bayesian graph, and more. The `apply_naive_bayes` method of the `Book` class is what orchestrates the generation of a Markov chain by selecting a random sample of sequential words of length `max_chains` to seed the Markov chain. The code generates a Markov chain from this seed for a specified number of iteration and prints the results.\n",
    "\n",
    "## Results\n",
    "Here is a sample of the program output with `max_chains = 15` (thus generating a seed of length 15 words) extended by 15 words:\n",
    "\n",
    "```\n",
    "Applying naive bayes to: Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\n",
    "--------\n",
    "Seed:\n",
    "\"me of success with those friends whom i am on the point of meeting may...\"\n",
    "Generated sentence:\n",
    "\"me of success with those friends whom i am on the point of meeting may i remember the time that there i cursed him had received the most of divine\"\n",
    "Actual:\n",
    "\"me of success with those friends whom i am on the point of meeting may i know the names and residence of those friends i paused this i thought was\"\n",
    "```\n",
    "\n",
    "Comparing the post-seed text of the generated sentence and the actual sentence taken from the book, we can see that with a short length of words, not many meaningful conclusions can be drawn about the overall behaviour of the model:\n",
    "\n",
    "<img src=\"results/results_15_words.png\">\n",
    "\n",
    "Performing this same analysis on a generated sequence length of 100 words, and the results are more telling:\n",
    "\n",
    "<img src=\"results/results_100_words.png\">\n",
    "\n",
    "What this chart reveals is that the model often over-predicts common words like \"the\" and \"and\" compared to the actual text, and it predicts many words that are not in the actual sentence; this is to be expected, as any deviation from the actual text is compounded the longer the Markov chain is. An important success of the model is that it follows the overal-trend of word frequency in the document: because the x-axis labels are ordered from most-common in the whole text body to least-common in the whole text body from left to right respectively, we would expect the above histograms to be right-skewed - and indeed they are.\n",
    "\n",
    "Adjusting the `max_chains` paramter to a value of 2 (effectively simulating bi-grams), the tendancy to over-predict common words is mitigated, but results in more spikes in the graph for the less-common words. Qualitatively (and in my opinion), the results with `max_chains=2` are less coherent:\n",
    "\n",
    "<img src=\"results/results_100_words_max_chain_2.png\">\n",
    "\n",
    "```\n",
    "Applying naive bayes to: Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\n",
    "--------\n",
    "Seed:\n",
    "\"which fell...\"\n",
    "Generated sentence:\n",
    "\"which fell however finding that and hunger when unprejudiced by an interview with contempt yet taken possession\"\n",
    "Actual:\n",
    "\"which fell into her hands she heard of the exile of her lover and learnt the name\"\n",
    "```\n",
    "\n",
    "## Future Work\n",
    "My first course of action to improve upon this project would be to implement unit testing to ensure that all of the probabilities are being calculated correctly; while I manually probed data with my debugger to ensure on a preliminary level that probabilities were being calculated correctly, it was not as rigorous as fully-implemented unit testing.\n",
    "\n",
    "Following this, a good next step would be integrating this functionality with optimized model hyperparameters (per [1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://arxiv.org/pdf/1803.00985.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Probabalistic ML Mini-Project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
